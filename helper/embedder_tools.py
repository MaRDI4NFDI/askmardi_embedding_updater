import logging
import os
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from typing import Dict, List, Optional

from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings

from helper.constants import DOCUMENT_TYPE_CRAN, DOCUMENT_TYPE_MARDI, DOCUMENT_TYPE_PUBLICATION


class EmbedderTools:
    """Utilities for loading documents and generating semantic embeddings."""

    def __init__(
        self,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        chunk_params: Optional[Dict] = None,
        model_kwargs: Optional[Dict] = None,
    ) -> None:
        """
        Initialize the embedding model and semantic chunker.

        Args:
            model_name: Hugging Face embedding model identifier.
            chunk_params: Optional overrides for SemanticChunker configuration.
            model_kwargs: Optional kwargs forwarded to HuggingFaceEmbeddings (e.g., device="cpu").
        """
        model_kwargs = model_kwargs or {"device": "cpu"}
        self.embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)
        # Target ~800â€“1800 char chunks with light overlap to fit 10 chunks into a 4k token window.
        default_chunk_params: Dict = {
            "min_chunk_size": 900,
            "target_max_chunk_size": 1600,
            "buffer_size": 200,
            "breakpoint_threshold_type": "percentile",
            "breakpoint_threshold_amount": 80,
            "add_start_index": True,
        }
        self.chunk_params = {**default_chunk_params, **(chunk_params or {})}
        # Only pass parameters supported by SemanticChunker to avoid unexpected kwargs.
        chunker_kwargs = {
            "min_chunk_size": self.chunk_params["min_chunk_size"],
            "buffer_size": self.chunk_params["buffer_size"],
            "breakpoint_threshold_type": self.chunk_params["breakpoint_threshold_type"],
            "breakpoint_threshold_amount": self.chunk_params["breakpoint_threshold_amount"],
            "add_start_index": self.chunk_params["add_start_index"],
        }
        self.chunker = SemanticChunker(embeddings=self.embeddings, **chunker_kwargs)
        self.embedding_dimension = len(self.embed_text("dimension probe"))

    def load_pdfs(self, folder_path: str) -> List[Document]:
        """
        Load all PDF files from a directory into LangChain documents.

        Args:
            folder_path: Directory containing PDF files.

        Returns:
            List[Document]: Documents with page content and metadata.
        """
        all_docs: List[Document] = []
        for filename in os.listdir(folder_path):
            if filename.endswith(".pdf"):
                loader = PyPDFLoader(os.path.join(folder_path, filename))
                all_docs.extend(loader.load())
        return all_docs

    def load_pdf_file(self, file_path: str) -> List[Document]:
        """
        Load a single PDF file into LangChain documents, which represent
        the individual pages.

        Args:
            file_path: Absolute or relative path to the PDF file.

        Returns:
            List[Document]: Documents with page content and metadata.
        """
        loader = PyPDFLoader(file_path)
        return loader.load()

    def split_and_filter(
            self,
            documents: List[Document],
            min_length: Optional[int] = None,
            timeout_seconds: int = 100) -> List[Document]:
        """
        Create semantic chunks and flag size outliers for visibility.

        Args:
            documents: Source documents to split.
            min_length: Optional lower bound for logging chunk size drift.
            timeout_seconds: Maximum time allowed for semantic chunking.

        Returns:
            List[Document]: Semantic chunks generated by the splitter.
        """
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(self.chunker.split_documents, documents)
            try:
                chunks = future.result(timeout=timeout_seconds)
            except FuturesTimeoutError as exc:
                future.cancel()
                raise TimeoutError(f"Semantic chunking timed out after {timeout_seconds}s") from exc
        expected_min = min_length or self.chunk_params["min_chunk_size"]
        expected_max = self.chunk_params.get("target_max_chunk_size", expected_min * 2)

        adjusted_chunks: List[Document] = []
        for chunk in chunks:
            if len(chunk.page_content) > expected_max:
                adjusted_chunks.extend(self._split_too_long(chunk, expected_max, expected_min))
            else:
                adjusted_chunks.append(chunk)

        outlier_lengths = [
            len(chunk.page_content)
            for chunk in adjusted_chunks
            if len(chunk.page_content) < expected_min or len(chunk.page_content) > expected_max
        ]
        if outlier_lengths:
            logging.getLogger(__name__).debug(
                "SemanticChunker produced %s outlier chunks out of %s (len min=%s max=%s) outside expected [%s, %s]",
                len(outlier_lengths),
                len(adjusted_chunks),
                min(outlier_lengths),
                max(outlier_lengths),
                expected_min,
                expected_max,
            )
        return adjusted_chunks

    @staticmethod
    def _split_too_long(doc: Document, expected_max: int, expected_min: int) -> List[Document]:
        """
        Split oversized chunks into roughly expected_max-sized pieces.

        Args:
            doc: Document chunk to split.
            expected_max: Target upper bound for chunk length.
            expected_min: Target lower bound for chunk length (used for overlap sizing).

        Returns:
            List[Document]: Smaller documents after splitting the oversized input.
        """
        content = doc.page_content
        meta = doc.metadata
        result: List[Document] = []
        if len(content) <= expected_max:
            return [doc]
        # naive fixed-size splitting with small overlap to preserve context
        step = expected_max
        overlap = min(100, expected_min // 4)
        start = 0
        while start < len(content):
            end = min(len(content), start + step)
            segment = content[start:end]
            result.append(Document(page_content=segment, metadata=meta.copy()))
            if end == len(content):
                break
            start = end - overlap
        return result

    def embed_text(self, text: str, prefix: str = "") -> List[float]:
        """
        Generate an embedding vector for the provided text.

        Args:
            text: Text to embed.
            prefix: Optional short prefix to prepend before embedding.

        Returns:
            List[float]: Embedding vector.
        """
        if prefix:
            text = f"{prefix}{text}"
        return self.embeddings.embed_query(text)

    def embed_document(self, doc: Document) -> List[float]:
        """
        Embed a LangChain Document using a short provenance prefix applied at embed time.

        Args:
            doc: Document whose content and metadata will be embedded.

        Returns:
            List[float]: Embedding vector for the document.
        """
        prefix = self._build_embedding_prefix(doc.metadata)
        # Prefix is applied only to the embedded text; the stored payload remains the original page_content.
        text = f"{prefix}{doc.page_content}"
        logging.getLogger(__name__).debug(
            "Embedding document with prefix-applied text (truncated): %s",
            text[:100],
        )
        return self.embeddings.embed_query(text)

    def _build_embedding_prefix(self, metadata: Dict) -> str:
        """
        Build a short, stable prefix for embedding based on document_type.

        Args:
            metadata: Metadata dictionary attached to the document.

        Returns:
            str: Prefix string to prepend before embedding, or empty string when not applicable.
        """
        doc_type = metadata.get("document_type")
        if doc_type == DOCUMENT_TYPE_CRAN:
            pkg = metadata.get("package")
            if pkg:
                return f"Package {pkg}. "
            return ""
        if doc_type == DOCUMENT_TYPE_MARDI:
            qid = metadata.get("qid")
            if qid:
                return f"MaRDI item {qid}. "
            return ""
        if doc_type == DOCUMENT_TYPE_PUBLICATION:
            title = metadata.get("title")
            if title:
                return f"Publication {title}. "
            return ""
        return ""
